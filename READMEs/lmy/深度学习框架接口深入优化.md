# 深度学习框架接口深入优化（一）

[TOC]

##背景

继上次实现一个基础的优化版本完成后，我们提到了进一步优化的思路，而那之后又有了一些新的想法，在这里重新整理一下

- 上一个次更新进度中的内容本身是框架性质的东西，还不能用到项目中，需要对之前提到的draw_detection函数和test_detector函数进行修改
- 上一次实现的并行并不充分，计算量更大的部分——letterbox_image放置没管，依旧是串行的实现

这一次我们的工作就是将整个系统连成可用的框架，然后进行更深层次的并行优化

##项目整理

上一次更新之后我们又尝试了很多工作，产生了不少代码，所以对这个模块的代码进行了整理，内容如下

- pydarknet.c

  调用darknet的python拓展，就是我们上一次实现的接口，在那之后我有对接口进行了一些修缮，如下：

  ```C
  /*******************************************************
   *
   * 这个项目是讲darknet detector封装成Python Extention
   * 的工作，为了提高资源利用率，讲opencv的imagePython对象
   * 有效转化成darknet可直接利用的对象，避免反复磁盘访问，
   * 具体方法支持：
   *
   * PyObject -(pyopencv_to)-> Mat -(IplImage(Mat*))->
   * IplImage -(ipl_to_image)->Image(darknet原生结构)
   * 其中ipl_to_image有优化空间
   *
   *
   * update：
   * PyObject的类型是numpy的ndarray，而且numpy又有c++接口
   * 那么我们只要开发出一套C-numpy -> image的转换就可以了
   * 技术测试：tech_test.cpp
   *
   *******************************************************/

  #include "pydarknet.h"
  /**
   * @brief build_pyarr
   * 单独写出来，反正就是为了把int类型转换成一个可以返回的python对象
   * @param coordinates 表示边界的四维数组
   * @return 创建好的可以直接返回的Python对象
   */
  inline static PyObject* build_pyarr(int** coordinates)
  {
  	//int* i_data = calloc(batch_size*4, sizeof(int));
  	//printf("access : %d \n",coordinates[0][1]);
  	//memcpy(i_data, coordinates[0], sizeof(int)*4*batch_size);

  	npy_intp* dims = calloc(2,sizeof(npy_intp));
  	dims[0] = batch_size;
  	dims[1] = 4;

  	PyObject* ret = PyArray_SimpleNewFromData(2, dims, NPY_INT, coordinates[0]);
  	Py_INCREF(ret);
  	return ret;
  }

  #include <Python.h>
  /**
   * @brief pydarknet_detect
   * python 直接调用的函数，现在做一个过渡，真正的实现不要和Python接口这种东西混在一起
   * @param self
   * @param args
   * @return
   */
  static PyObject* pydarknet_detect(PyObject* self, PyObject* args)
  {
  	float 		thresh;
  	PyObject* 	parr;

  	PyArg_ParseTuple(args, "fO",&thresh, &parr);
  	//----------------------------------
  	//printf("Your thresh: %f\n",thresh);
  	//----------------------------------
  	PyArrayObject *ndarr = (PyArrayObject* ) parr;	//获得对象指针
  	/*
  	if(ndarr->nd != 4 
  	|| ndarr->dimensions[1] != IN_H
  	|| ndarr->dimensions[2] != IN_W
  	|| ndarr->dimensions[3] != 3
  	|| ndarr->data == NULL)
  	{
  		printf("fatal error: I don't know what happened to input data!\n");
  		printf("Your Pic:\n\tsize: %d * %d\n",ndarr->dimensions[1],ndarr->dimensions[2]);
  		exit(-1); //意思就是说这个numpy的nd数组肯定是4维的，提交前注释掉此分支
  	}
  	*/
  	batch_size = ndarr->dimensions[0];
  	if(!batch_size) Py_RETURN_NONE;
  	//----------------------------------
  	//printf("Batch with %d pics\n",batch_size);
  	//----------------------------------

  	//把核心逻辑放到别的文件里吧
  	int** coord = do_detection(ndarr->data, thresh);
  	/*
  		注意：这个coord千万别free。
  			这是用来创建Python对象的，free了就什么都没有了
  	*/

  	//cord进行一些处理，生成可以用来返回的对象
  	PyObject* pyArray = build_pyarr(coord);

  	batch_size = 0; //最后记得归0
  	return pyArray;
  }

  /**
   * @brief pydarknet_init_detector
   * 完成头文件中关键组件的初始化，同时调用不同实现的初始化函数
   *  datacfg	data文件
   *  cfgfile	cfg文件
   *  weightfile	权值文件
   */
  static PyObject* pydarknet_init_detector(PyObject* self, PyObject* args)
  {
  	char *datacfg, *cfgfile, *weightfile;
  	PyArg_ParseTuple(args, "sss", &datacfg, &cfgfile, &weightfile);
  	//---------------------------------------------------------------------
  	printf("data: %s\ncfg: %s\nweights: %s\n",datacfg, cfgfile, weightfile);
  	//---------------------------------------------------------------------

  	srand(2222222);
  	__init();	//根据功能不同调用不同的
  	list *options = read_data_cfg(datacfg);
  	char *name_list = option_find_str(options, "names", "data/names.list");
  	names = get_labels(name_list);
  	net = load_network(cfgfile, weightfile, 0);
  	set_batch_network(net, 1);
  	Py_RETURN_NONE;//用这个维持Python解释器环境的正常
  }

  /**
   * @brief pydarknet_finalize_detector
   * 当工作结束后回收资源用，并不是所有实现都用的着
   * @param self
   * @param args
   * @return
   */
  static PyObject* pydarknet_finalize_detector( PyObject* self, PyObject* args )
  {
  	__finalize();
  	Py_RETURN_NONE;
  }

  static PyMethodDef meth_list[] =
  {
  	{"detect", pydarknet_detect, METH_VARARGS},
  	{"init_detector", pydarknet_init_detector, METH_VARARGS},
  	{"finalize_detector", pydarknet_finalize_detector, METH_NOARGS}, 
  	{NULL, NULL, 0, NULL}
  };

  void initpydarknet()
  {
  	Py_InitModule("pydarknet", meth_list);
  	_import_array();
  }
  ```

  其中`__init`和`__finalize`是不同实现（并行、串行）所需的特定初始化流程，都有分别定义。

  同理，`do_detection`函数现在是识别的真正过程，返回识别结果

- det.c

  串行实现的版本，完成的是功能优化的正确性测试

- det_para.c

  并行优化版本，包含的是我们最终要使用的内容

##letterbox_image函数的深入剖析

在我们之前分析的时候我们仅仅把这个函数视为图像处理的一个简单步骤，直到最后总结工作的时候才发现这个部分的计算量比我们rbgbr（指图像存储方式转换）的计算量更大，我们首先来看一下这个函数

```C
image letterbox_image(image im, int w, int h)
{
    int new_w = im.w;
    int new_h = im.h;
    //保证图像宽高比不变，计算放缩后的宽高
    if (((float)w/im.w) < ((float)h/im.h)) {
        new_w = w;
        new_h = (im.h * w)/im.w;
    } else {
        new_h = h;
        new_w = (im.w * h)/im.h;
    }
    image resized = resize_image(im, new_w, new_h);//进行放缩
    image boxed = make_image(w, h, im.c);//创建空白图片
    fill_image(boxed, .5);
    //int i;
    //for(i = 0; i < boxed.w*boxed.h*boxed.c; ++i) boxed.data[i] = 0;
    embed_image(resized, boxed, (w-new_w)/2, (h-new_h)/2);//将放缩后的图片复制入空白图片正中央 
    free_image(resized);//释放放缩后的图片
    return boxed;
}
```

这里还涉及到两个函数，我们依次分析

```C
image resize_image(image im, int w, int h)
{
    image resized = make_image(w, h, im.c);   
    image part = make_image(w, im.h, im.c);
    int r, c, k;
    float w_scale = (float)(im.w - 1) / (w - 1);
    float h_scale = (float)(im.h - 1) / (h - 1);
    //为了保证访存效率，都是按照行遍历
    //重置宽度
    for(k = 0; k < im.c; ++k){
        for(r = 0; r < im.h; ++r){
            for(c = 0; c < w; ++c){
                float val = 0;
                if(c == w-1 || im.w == 1){
                    val = get_pixel(im, im.w-1, r, k);
                } else {	//重置方式
                    float sx = c*w_scale;	//利用新图片中的坐标找到原图中坐标的位置(一般不是整数)
                    int ix = (int) sx;		//将分数坐标分成整数和小数部分
                    float dx = sx - ix;
                    val = (1 - dx) * get_pixel(im, ix, r, k) + dx * get_pixel(im, ix+1, r, k);	//整数部分作为真正坐标，小数部分作为其和相邻像素的混合系数
                }
                set_pixel(part, c, r, k, val);
            }
        }
    }
    //重置高度
    for(k = 0; k < im.c; ++k){
        for(r = 0; r < h; ++r){
            float sy = r*h_scale;
            int iy = (int) sy;
            float dy = sy - iy;
            for(c = 0; c < w; ++c){	//将主数据放入
                float val = (1-dy) * get_pixel(part, c, iy, k);
                set_pixel(resized, c, r, k, val);
            }
            if(r == h-1 || im.h == 1) continue;
            for(c = 0; c < w; ++c){//将额外数据加上
                float val = dy * get_pixel(part, c, iy+1, k);
                add_pixel(resized, c, r, k, val);
            }
        }
    }

    free_image(part);
    return resized;
}
```

这个过程的计算量达到惊人，后来经过资料查找知道了这个算法是二线性插值算法，就为了这么一个算法还专门产生了一个中间结果，容易理解是真的，但是效率很难说，这个函数不适合我们的原因有以下几点

- 输入参数是一个image结构体，而我们的处境是输入只能是numpy中ndarray的data域
- 其中创建了中间结果但很快释放掉了，虽然可能实现了顺序访存，但是申请释放的占用的时间不敢恭维

然后是embed_image

```c
void embed_image(image source, image dest, int dx, int dy)
{
    int x,y,k;
    for(k = 0; k < source.c; ++k){
        for(y = 0; y < source.h; ++y){
            for(x = 0; x < source.w; ++x){
                float val = get_pixel(source, x,y,k);
                set_pixel(dest, dx+x, dy+y, k, val);
            }
        }
    }
}
```

这个函数本身比较简单，就是将一张图片按照偏移量写入另一张图片。

> 另外，这中间的set_pixel和get_pixel函数都是在从数组中取值之前调用了assert来保证索引合法

综合看来，`letterbox_image`这个函数是这样的：

- 创建了一个图片，存储resize的结果，后来被释放掉
- resize过程中创建了图片存储中间结果，后来也被释放掉
- 创建了用来作为返回值的图片，把resize的图片“嵌”了进去
- 所有的图片处理全部都是3层for循环

而且在整个识别过程来看，结果是这样的：

- 输入了一个图片，这个图片被resized
- 输入图片被释放掉，resize的图片被拿去做识别

##图片预处理优化

###思路简述

经过上面分析，整个优化思路就比较明确了

- 尽量消除中间结果，一步到位
- 顾及访存的局部性，尽量充分利用cache

###保证正确性基础的实现

第一阶段的目标就是消除中间过程，即实现：

- 输入numpy::ndarray的data域，输出可以直接用来识别的图像

为了实现这一目标，我们需要做到

- 从上述data域中获得一个像素的一个通道的值

  这一点在了解了numpy::ndarray的定义以后实际上就很好实现了，stride域定义了每一个维度的跳步，我们可以用这个跳步定义到我们每一个数据元素（float64格式）的字节地址，使用memcpy就可以直接得到这个数据

  上述思路已经体现在了最初的并行实现中，但是后来发现了更加简单的算法——反正data域里面是按照float64格式（对应C的double）存储的，那我直接把data域这个char指针转换成double指针就可以了

- 将这个值放入输出图像的正确位置

  这个问题涉及以下几点：

  - 映射方式的不同：opencv的python接口读出的numpy对象是按照h,w,c的顺序存储数据的，而darknet中image类型是按照c,h,w的映射顺序存储的，而且通道顺序一个是rgb，一个是bgr

    > 认清这个现实的工作也花了不少时间，甚至用到了“智商检测图”（用红绿蓝三色写的rgb三个字母），这里不再赘述

  - 数据应该放到的地方并不是取出的h,w,c三坐标：因为我们希望消除中间结果，所以取出的值一方面会被用于二线性插值的计算，另一方面，计算得到的结果会被加上偏移量放到新图片中

  认识到这些问题后，我们首先应该验证的是二线性插值算法应该怎样在不产生中间结果的前提下实现：

  - 要点在于一次计算出新图片中的一个值：涉及四个点，按照作者的实现，首先对w坐标加权求和，然后对h坐标加权求和
  - 四个点的坐标需要通过缩放比例来计算，算出的值（一般是分数）采用顶函数和底函数即可得出四个点的坐标

  二线性插值实现

  ```C
  static void set_pixel(image m, int x, int y, int c, float val)
  {
      if (x < 0 || y < 0 || c < 0 || x >= m.w || y >= m.h || c >= m.c) return;
      assert(x < m.w && y < m.h && c < m.c);
      m.data[c*m.h*m.w + y*m.w + x] = val;
  }

  static void add_pixel(image m, int x, int y, int c, float val)
  {
      if (x < 0 || y < 0 || c < 0 || x >= m.w || y >= m.h || c >= m.c) return;
      assert(x < m.w && y < m.h && c < m.c);
      m.data[c*m.h*m.w + y*m.w + x] += val;
  }

  static float get_pixel(image m, int x, int y, int c)
  {
  	//printf("getting [%d, %d]\n",y,x);
      assert(x < m.w && y < m.h && c < m.c);
      return m.data[c*m.h*m.w + y*m.w + x];
  }
  ```

  这些都是辅助函数，完成最基本的取、存、改功能

  ```c
  static inline float get_val(image im, int x, int y, int w, int h, int z, float dx, float dy)
  {
  	float val = get_pixel(im, x, y, z);
  	if(x == w-1 && y == h-1) return val;
  	if(x == w-1)
  	{
  		return val*(1-dy) + dy*get_pixel(im, x, y+1, z);
  	}
  	if(y == h-1)
  	{
  		return val*(1-dx) + dx*get_pixel(im, x+1, y, z);
  	}
  	val = val*(1-dx) + dx*get_pixel(im, x+1, y, z);
  	float val1 = (1-dx)*get_pixel(im, x, y+1, z) + dx*get_pixel(im, x+1, y+1, z);
  	return val*(1-dy)+val1*dy;
  }
  ```

  这一步就是二线性插值的实现

  ```c
  image resize_image_(image im, int w, int h)
  {
  	int new_w = im.w;
      int new_h = im.h;
      if (((float)w/im.w) < ((float)h/im.h)) {
          new_w = w;
  		new_h = (im.h * w)/im.w;
      } else {
          new_h = h;
          new_w = (im.w * h)/im.h;
      }
      
  	image resized = make_image(w, h, im.c);   
      int x, y, z;
      float w_scale = (float)(im.w - 1) / (new_w - 1);
      float h_scale = (float)(im.h - 1) / (new_h - 1);
      for(z = 0; z < im.c; z++)
      {
      	for(y = 0; y < h; y++)
      	{
      		float sy = (y - (h-new_h)/2)*h_scale;
              int iy = (int) sy;
              float dy = sy - iy;
              //printf("new line: %d -> old line: %d\n", y, iy);
      		if(y < (h-new_h)/2 || y >= (h+new_h)/2)
      		{
      			for(x = 0; x < w; x++)
      				set_pixel(resized, x, y, z, .5);
  				continue;
  			}
  			for(x = 0; x < w; x++)
  			{
  				if(x < (w-new_w)/2 || x >= (w+new_w)/2 )
  				{
  					set_pixel(resized, x, y, z, .5);
  				} else {
  					float val = 0;
  				 	float sx = (x - (w-new_w)/2)*w_scale;
  					int ix = (int) sx;	
  					float dx = sx - ix;
  					val = get_val(im, ix, iy, im.w, im.h, z, dx, dy);
  					set_pixel(resized, x, y, z, val);
  			   	}
  	        }
  	    }
      }
      return resized;
  }
  ```

  最终实现，这一系列函数都只是image结构体之间的转换，目的就是单纯的技术测试，同时探究更优的方法。我确实还设计了一个优化方法

  ```c
  image __resize_image(image im, int w, int h)
  {
  	image resized = make_image(w, h, im.c);   
      int x, y, z;
      float w_scale = (float)(im.w - 1) / (w - 1);
      float h_scale = (float)(im.h - 1) / (h - 1);
      for(z = 0; z < im.c; z++)
      {
      	int last_iy = -1;
      	float last_dy = 0;
      	for(y = 0; y < h; y++)
      	{
      		float sy = y*h_scale;
              int iy = (int) sy;
              float dy = sy - iy;
  		
  			//printf("new line: %d -> old line: %d\n", y, iy);
  			for(x = 0; x < w; x++)
  			{
                  float val = 0;
                  float sx = x*w_scale;
                  int ix = (int) sx;	
                  float dx = sx - ix;
                  val = get_pixel(im, ix, iy, z);
                  if(x != w-1)
  				{
  					val = val*(1-dx) + dx*get_pixel(im, ix+1, iy, z);
  				}
  				
  				if(last_iy+1 == iy && y) 
  				{
  					add_pixel(resized, x, y-1, z, val*last_dy);
  				}
  				else
  				{
  					float val1 = get_pixel(im, ix, last_iy+1, z);
  				    if(x != w-1)
  					{
  						val1 = val1*(1-dx) + dx*get_pixel(im, ix+1, last_iy+1, z);
  					}
  					add_pixel(resized, x, y-1, z, val1*last_dy);
  				}
  				set_pixel(resized, x, y, z, val*(1-dy));   
  			}
  			
  			last_iy = iy;
  			last_dy = dy;
      	}
      }
      return resized;
  }
  ```

  这个函数中，我完全按照连续原则进行内存读写，这样又多了许多复杂的考虑——新图片中列坐标对应到老图片中列坐标有时连续有时不连续，对连续的情况，我们应当进行优化，即处理一行的同时向另一行写入。但遗憾的是这个实现效果并不好，还不如上面那个最简单的

  测试代码

  ```c
  int main()
  {
  	image im = load_image("results/images/0.jpg", 0, 0, 3);
  	double start = what_time_is_it_now();
  	image resized;
  	for(int i = 0; i < 1000; i++)
  	{
  		resized = resize_image_(im, 416, 416);
  		//resized = letterbox_image(im, 416, 416);
  	}
  	printf("time elapses: %f\n",what_time_is_it_now() - start);
  	save_image(resized, "res");
  }
  ```

  处理1000张图片，原始方法的时间是2.9s，第一个改进版时间是1.9s，而优化版时间是2.4s，无疑是失败的。分析原因，很可能是add_pixel函数的运算既读又写，实际上增加了大量的访存

### 实用实现与优化

接下来我实现了串行处理的实用过程

```c
#define GET_PIXEL(ptr,w,h,c) ((float)(ptr)[(h)*3*IN_W + (w)*3 + (c)])

/**
 * @brief get_val
 * @param ptr
 * @param x
 * @param y
 * @param z
 * @param dx
 * @param dy
 * @return
 */
//static inline float get_val(double* ptr, int x, int y, int z, float dx, float dy)
static inline float get_val(float* ptr, int x, int y, int z, float dx, float dy)

{
	float val = GET_PIXEL(ptr, x, y, z);
	//printf("[ %d, %d, %d] = %f\n",x,y,z, val);
	if(x == IN_W-1 && y == IN_H-1) return val;
	if(x == IN_W-1)
	{
		return val*(1-dy) + dy*GET_PIXEL(ptr, x, y+1, z);
	}
	if(y == IN_H-1)
	{
		return val*(1-dx) + dx*GET_PIXEL(ptr, x+1, y, z);
	}
	val = val*(1-dx) + dx*GET_PIXEL(ptr, x+1, y, z);
	float val1 = (1-dx)*GET_PIXEL(ptr, x, y+1, z)+ dx*GET_PIXEL(ptr, x+1, y+1, z);
	return val*(1-dy)+val1*dy;
}


/**
 * @brief our_letterbox_image
 * 重新实现的串行大小调整算法，快一点
 * @param im
 * @param w
 * @param h
 * @return
 */
static inline image our_letterbox_image(double* conv_ptr, int w, int h)
{
	int new_w = IN_W;
	int new_h = IN_H;
	if (((float)w/IN_W) < ((float)h/IN_H)) {
		new_w = w;
		new_h = (IN_H * w)/IN_W;
	} else {
		new_h = h;
		new_w = (IN_W * h)/IN_H;
	}

	image resized = make_image(w, h, 3);
	int x, y, z;

//	image tmp;
//	tmp.c = 3;
//	tmp.h = IN_H;
//	tmp.w = IN_W;
//	tmp.data = buf;
//	save_image(tmp, "tmp");
	//--------------------------------------------------

	float w_scale = (float)(IN_W - 1) / (new_w - 1);
	float h_scale = (float)(IN_H - 1) / (new_h - 1);
	for(z = 0; z < 3; z++)
	{
		for(y = 0; y < h; y++)
		{
			float sy = (y - (h-new_h)/2)*h_scale;
			int iy = (int) sy;
			float dy = sy - iy;
			//printf("new line: %d -> old line: %d\n", y, iy);
			if(y < (h-new_h)/2 || y >= (h+new_h)/2)
			{
				for(x = 0; x < w; x++)
					resized.data[z*w*h + y*w + x] = .5f;
				continue;
			}
			for(x = 0; x < w; x++)
			{
				if(x < (w-new_w)/2 || x >= (w+new_w)/2 )
				{
					resized.data[z*w*h + y*w + x] = .5f;
				} else {
					float sx = (x - (w-new_w)/2)*w_scale;
					int ix = (int) sx;
					float dx = sx - ix;
					resized.data[z*w*h + y*w + x] = get_val(conv_ptr, ix, iy, z, dx, dy);
					//printf("[ %d, %d, %d] = %f\n",z,y,x,resized.data[z*w*h + y*w + x]);
				}
			}
		}
	}
	free(buf);
	return resized;
}
```

而对应的，我们上一次也提到要修改的test_detector，被改成了这个样子：

```c
static void test_detector(image* imgs, float thresh, int** coords)
{
	//init_detector(...)

	set_batch_network(net, 1);
	//double time;
	float nms=.45;
	
	for(int i=0; i < batch_size; ++i){

		//image im = imgs[i];
		//image sized = our_letterbox_image(im, net->w, net->h);
		/********************************
		 * 改性方法，如需恢复，注释掉下面的块
		 * ******************************/
		//---------------------------------
		image im;
		im.h = IN_H;
		im.w = IN_W;
		image sized = imgs[i];
		//save_image(sized, "res");
		//---------------------------------
		layer l = net->layers[net->n-1];

		float *X = sized.data;
		//time=what_time_is_it_now();
		network_predict(net, X);
		//printf("%s: Predicted in %f seconds.\n", path, what_time_is_it_now()-time);
		int nboxes = 0;
		detection *dets = get_network_boxes(net, im.w, im.h, thresh, thresh, 0, 1, &nboxes);
		//printf("%d\n", nboxes);
		//if (nms) do_nms_obj(boxes, probs, l.w*l.h*l.n, l.classes, nms);
		if (nms) do_nms_sort(dets, nboxes, l.classes, nms);
		draw_detections(im, dets, nboxes, thresh, names, l.classes, coords[i]);

		free_detections(dets, nboxes);

		//free_image(im);
		//save_image(sized,"res");
		free_image(sized);

		//---------------------------------------------
		//printf("coords: %d, %d, %d, %d\n", coordinates[i][0],coordinates[i][1],coordinates[i][2],coordinates[i][3]);
		//---------------------------------------------
	}
	
	return coords;
}
```

而draw_detection被改成了这样

```c
void draw_detections(image im, detection *dets, int num, float thresh, char **names, int classes, int *coordinates)
{
    int i,j;
    float max_prob = dets[0].prob[0];
    //printf("max_prob init: %.8f\n", max_prob); zcq
    int max_det = -1;
    int max_class = -1;
    for(i = 0; i < num; ++i){
        for(j = 0; j < classes; ++j){
//printf("prob: %.8f\n", dets[i].prob[j]);
            if(dets[i].prob[j] > max_prob){
                max_prob = dets[i].prob[j];
//printf("prob: %.8f\n", dets[i].prob[j]);
                max_det = i;
                max_class = j;
            }    
        }
    }
    if(max_det >= 0){
//printf("max_det index: %d\n", max_det); 
            box b = dets[max_det].bbox;
            //printf("%f %f %f %f\n", b.x, b.y, b.w, b.h);

            int left  = (b.x-b.w/2.)*im.w;
            int right = (b.x+b.w/2.)*im.w;
            int top   = (b.y-b.h/2.)*im.h;
            int bot   = (b.y+b.h/2.)*im.h;
//printf("left: %d\n", left);
//printf("right: %d\n", right);
            coordinates[0] = left;
            coordinates[1] = right;
            coordinates[2] = top;
            coordinates[3] = bot;
    }
}
```

总而言之就是去掉了大量的画框过程

而识别逻辑`do_detection`如下

```C
int** do_detection(char* data, float thresh)
{
	int		pic;//, h, w, c;
	double*	float_buf = (double*) data;

	image* imgs = calloc(batch_size, sizeof(image));

	//initialize coordinates
	int **coords = (int**)calloc(batch_size, sizeof(int*));
	int *real_coord = (int*) calloc(batch_size*4, sizeof(int));
	for(int j = 0; j < batch_size; ++j)
	{
		coords[j] = real_coord + 4*j;
	}

	/************************************
	这里通道的对应关系十分诡异：
	 - 0 : image_b, numpy_r
	 - 1 : image_g, numpy_g
	 - 2 : image_r, numpy_b
	*************************************/
	for(pic = 0; pic < batch_size; pic++)
	{
//		imgs[pic] = make_image(IN_W, IN_H, 3);
//		for(c = 2; c >= 0; c--)
//		{
//			for(h = 0; h < IN_H; h++)
//				for(w = 0; w < IN_W; w++)
//				{
//					imgs[pic].data[c*IN_W*IN_H + h*IN_W + w] = (float) float_buf[h*3*IN_W+w*3+2-c]/255.;
//					//printf("pic %d:[ %d, %d, %d] = %f\n",pic,h,w,c,imgs[pic].data[c*IN_W*IN_H + h*IN_W + w]);
//				}
//		}

		imgs[pic] = our_letterbox_image(float_buf+pic*IN_H*IN_W*3, NET_SIZE, NET_SIZE);
//		save_image(imgs[pic], "res");
	}
	//----------------------------------
	//printf("Pics conversion complete!\n");
	//----------------------------------
	//初始化完成，接下来调用识别
	test_detector(imgs, thresh, coords);
	free(imgs);
	return coords;
}
```

至此，整个模块已经可以实现通过python调用并返回numpy数组了

下面考虑优化：

- GET_PIXEL这个宏访存的方式实在太混乱，而且调用频繁，很可能影响性能
- 解决方案：使用一个中间缓冲区，存储转换的中间结果，不是创建一个image对象，而是简单地使用float数组，存入的时候按照ndarray局部性访存，放缩时按照image局部性访存


实现结果

```c
#define GET_PIXEL(ptr,w,h,c) ((ptr)[(c)*IN_H*IN_W + (h)*IN_W + (w)])
static inline image our_letterbox_image(double* conv_ptr, int w, int h)
{
	int new_w = IN_W;
	int new_h = IN_H;
	if (((float)w/IN_W) < ((float)h/IN_H)) {
		new_w = w;
		new_h = (IN_H * w)/IN_W;
	} else {
		new_h = h;
		new_w = (IN_W * h)/IN_H;
	}

	image resized = make_image(w, h, 3);
	int x, y, z;

	//buffering solution
	//经过验证，这样确实快
	float* buf = malloc(sizeof(float)*IN_H*IN_W*3);

	for(y = 0; y < IN_H; y++)
		for(x = 0; x < IN_W; x++)
			for(z = 2; z >= 0; z--)
				buf[z*IN_W*IN_H + y*IN_W + x] = (float) conv_ptr[y*3*IN_W+x*3+2-z]/255.;

//	image tmp;
//	tmp.c = 3;
//	tmp.h = IN_H;
//	tmp.w = IN_W;
//	tmp.data = buf;
//	save_image(tmp, "tmp");
	//--------------------------------------------------

	float w_scale = (float)(IN_W - 1) / (new_w - 1);
	float h_scale = (float)(IN_H - 1) / (new_h - 1);
	for(z = 0; z < 3; z++)
	{
		for(y = 0; y < h; y++)
		{
			float sy = (y - (h-new_h)/2)*h_scale;
			int iy = (int) sy;
			float dy = sy - iy;
			//printf("new line: %d -> old line: %d\n", y, iy);
			if(y < (h-new_h)/2 || y >= (h+new_h)/2)
			{
				for(x = 0; x < w; x++)
					resized.data[z*w*h + y*w + x] = .5f;
				continue;
			}
			for(x = 0; x < w; x++)
			{
				if(x < (w-new_w)/2 || x >= (w+new_w)/2 )
				{
					resized.data[z*w*h + y*w + x] = .5f;
				} else {
					float sx = (x - (w-new_w)/2)*w_scale;
					int ix = (int) sx;
					float dx = sx - ix;
					resized.data[z*w*h + y*w + x] = get_val(buf, ix, iy, z, dx, dy);//get_val(conv_ptr, ix, iy, z, dx, dy);
					//printf("[ %d, %d, %d] = %f\n",z,y,x,resized.data[z*w*h + y*w + x]);
				}
			}
		}
	}
	free(buf);
	return resized;
}
```

测试得到的结果确实不错，这个步骤的并行方式也进行了实现，如下

```c
static void* thread_conv_pic(void* args)
{
	conv_t *task = (conv_t*) args;
	
	double*	conv_ptr;
	int		x,y;
	int		src_channel = 2 - task->dst_channel;
	float*	buf = malloc(sizeof(float)*IN_H*IN_W);
	int		new_w = IN_W;
	int		new_h = IN_H;

	if (((float)NET_SIZE/IN_W) < ((float)NET_SIZE/IN_H)) {
		new_w = NET_SIZE;
		new_h = (IN_H * NET_SIZE)/IN_W;
	} else {
		new_h = NET_SIZE;
		new_w = (IN_W * NET_SIZE)/IN_H;
	}
	float w_scale = (float)(IN_W - 1) / (new_w - 1);
	float h_scale = (float)(IN_H - 1) / (new_h - 1);

	while(1)
	{
		sem_wait(&task->next_loop);
		//printf("thread %d started new work\n", task->dst_channel);
		conv_ptr = (double*)task->src;
		if(task->src == -1)
		{
			free(buf);
			return NULL;
		}/*
		for(h = 0; h < IN_H; h++)
			for(w = 0; w < IN_W; w++)
			{
				//memcpy((void*)&float_buf, task->src + h*step_h + w*step_w + src_channel*step_c, 8);
				task->dest[task->dst_channel*IN_W*IN_H + h*IN_W + w] = (float) conv_ptr[h*3*IN_W+w*3+src_channel]/255.;
				//--------------------------------
				//printf("giving %f on [%d, %d, %d]\n", task->dest[task->dst_channel*IN_W*IN_H + h*IN_W + w], task->dst_channel, h, w);
				//--------------------------------
			}
			//--------------------------
			//printf("thread %d conv done....\n", task->dst_channel);
			//--------------------------	
		*/
		//double time = what_time_is_it_now();
		for(y = 0; y < IN_H; y++)
			for(x = 0; x < IN_W; x++)
				buf[y*IN_W + x] = (float) conv_ptr[y*3*IN_W+x*3+src_channel]/255.;
		//-----------------------------------------
		//printf("thread %d conv done....\n", task->dst_channel);
		//-----------------------------------------
		for(y = 0; y < NET_SIZE; y++)
		{
			if(y < (NET_SIZE-new_h)/2 || y >= (NET_SIZE+new_h)/2)
			{
				for(x = 0; x < NET_SIZE; x++)
					task->dest[task->dst_channel*NET_SIZE*NET_SIZE + y*NET_SIZE + x] = .5f;
				continue;
			}
			float sy = (y - (NET_SIZE-new_h)/2)*h_scale;
			int iy = (int) sy;
			float dy = sy - iy;
			for(x = 0; x < NET_SIZE; x++)
			{

				float sx = (x - (NET_SIZE-new_w)/2)*w_scale;
				int ix = (int) sx;
				float dx = sx - ix;
				task->dest[task->dst_channel*NET_SIZE*NET_SIZE + y*NET_SIZE + x] = get_val(buf, ix, iy, task->dst_channel, dx, dy);//get_val(ptr, ix, iy, z, dx, dy);

			}
		}
		//等所有线程都完成了工作，告诉主线程准备好一张图片了
		pthread_barrier_wait(&conv_barrier);
		
		if(task->dst_channel == src_channel) //选一个代表报告图片完成
		{
			//----------------------
//			image tmp;
//			tmp.c = 3;
//			tmp.h = NET_SIZE;
//			tmp.w = NET_SIZE;
//			tmp.data = task->dest;
//			save_image(tmp, "tmp");
			//-----------------------
			sem_post(&conv_done);
			//--------------------------
			//printf("conv done for now, taking %f secs\n",what_time_is_it_now() - time);
			//--------------------------
		}		
	}
	return NULL;
}
```

同样的，在并行方式下，这个实现也得到了由于不缓冲方式的结果

##小结

到目前为止，优化的工作进行的比较顺利，目前还不排除有进一步优化的可能性，现阶段的目标是将这一步优化做到极致，原因是很明确的：

- 这个识别是需要在服务器上运行的，如果识别性能不佳，会影响到后续的处理